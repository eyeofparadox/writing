
CIS319 Captured Posts

A computer is nothing more than a set of resources for doing creative or productive work, and sometimes to relax and entertain yourself, but there is always something else that will take advantage of those resources and anything that can be gained by exploiting them, and even a few that operate on pure malice.

I do not know if there is a shred of truth to the suspicion, but I have a theory that some malware is created to provoke users into purchasing security software, a kind of pre-emptive job security. I am an artist and a writer who has not had much success in finding a market for my talents, so I have an understanding of what motivates someone to invest time and effort in his or her craft -- profit motive is the big one. My own creativity prompts me to write and draw, but I have long known that I need to be able to derive an income from my work in order to be able to devote the kind of time needed to excel in my craft. The same is true of a technical creator or programmer. Now, programming is different in that it is catalytic. A program is designed to do something, to interact, so a program can be used as an intelligent tool. It is a little bit like having demons at your disposal, because they can do work but have no conscience. The general public is not really aware of how sophisticated fuzzy logic, artificial intelligence is getting. We're decades away from something we would think of as consciously intelligent, but we are well into the realm of super-adaptive programming matrices that can simulate thoughtful action using complex decision-making and interactive learning databases.

My idea of security involves keeping sensitive information offline on a separate system, disconnecting from peripherals entirely when they are not in immediate use and securing my online system with firewall software that restricts access, anti-spy software that notifies me of all registry changes and having a security expert as a friend and mentor. I also keep my browser updated and try to avoid accessing sources of suspicious content, uploading and downloading only at trusted sites, deleting unsolicited email, and so forth. 

I use the Windows Firewall, AVG and SpyBot on my laptop, and my roommate has some proprietary network security he developed as a network security specialist; that puts me behind a network firewall as well. Everything is set to update automatically, so I always have to turn my computer on and wander off for a few minutes while AVG gets its daily update before I can do anything. For system maintenance I use the XP performance and maintenance utilties; I used to rely on Norton Utilities and Security, but the quality of the products has declined. I removed the pre-installed McAffee to get AVG, which proved more reliable in spite of being free. I have a USB external drive to back up to, but I only connect it to back up or retrieve files, or to store and transport graphic arts projects -- when I am working on an illustration I usually have several .psd files totaling 2 to 4 GB and prefer not to trust that much unique data to a flash drive that's big enough.

To make data stored in a magnetic media irretrievable takes a little effort and some creative thinking. If you have seen CSI episodes where they recover a serial number from a gun that had the original number filed off, then you would probably understand the forensic method employed to recover intentionally deleted or overwritten data. Massively overwritten data is far less likely to be recoverable, so a person who had time can destroy magnetic data that a person caught in a law enforcement raid would not. Because my mentor in IT was a security specialist, I have had conversations about this with him and come up with ideas of my own, which are based on techniques I have heard about, such as a wipe write, a script to destroy information by deletion, reformatting and then overwriting all sectors. My addition was to employ a fractal or moire write pattern to destroy traces of magnetic storage. Other methods involve more preparation, like encrypting all data using a dynamic protocol. One of the most ingenious methods I have heard of involved an extremely powerful, hand-wrapped electromagnet mounted inside the case and wired to a panic button. When it was actually used, it turned out that the person who made the magnet miscalculated and created a coil so powerful it imploded the case, the metal desk the case was sitting under and caused an EMP blackout in the surrounding neighborhood. Oops! I think that falls under the heading of physically destroying the drives.

I back up data in two to three different ways. I archive data by burning it to a CD and saving it to an external USB HD. I will also use a flashdrive to back up work in progress when I am writing, or when I am working on a graphics project that is only a few hundred MB. I have also been known to cold swap a spare internal hard drive in my desktop to do a massive backup or in preparation for serious maintenance, like reinstalling the OS. I have toyed with the idea of offsite storage, but the content I would want to store I would normally access only from my own computer and my portable external drive is small and light enough to cart around with my laptop. If I worked on the same content in more than one location regularly, I would be more inclined to use offsite storage online.

I am inclined to agree that interviewers tend to place greater emphasis on learning ability, the it is an unknown quality that forces them to weigh the risk of allowing people to prove themselves on the job. The will often use your resume to get an idea of your efficiency and reliability, however, and my resume can make me look like ADD on Crack! One one hand, my versatility has allowed me to perform competently in nearly any area I find myself, but on the other, my GID has caused mental and emotional breakdowns related to stress at work -- it has all the effects of a severe social anxiety disorder. Unfortunately, the condition is so poorly understood and so controversial that it is barely recognized as a debilitating condition. It often does more harm to disclose the condition. If that's overshare, my apologies; I know this makes people uncomfortable. As for my contracting specialty, I am a diagnostic troubleshooter with top-level abstract, theoretical design proficiency, an intuitive analyst. My practical knowledge base is sketchy (or I am just hyper-conscious of all the things I do not know), but I am able to see the big picture and break it down instantaneously. By myself, I am just an idea generator, but working in concert with technical experts I am a muse, able to communicate my own insights to inspire technical creativity. Put simply, I function at the level of the people I work with. I work best when I can work hands-on, able to gather information from all my senses, but my communications skills are enough to work remotely even through an inexperienced agent. My learning curve is high enough that I am not phased by the unknown; the benefit of knowledge is that it speeds up the process. Figuring things out on the spot is my proficiency. The down side is that I am never satisfied with my level of expertise and I do not like to sacrifice accuracy for haste. It does not matter how fast you work if means you cannot do the job right. I can handle repetitive tasks, as long as that is not the full scope of the job; repetitiveness is an essential part of problem solving. My artistic background helps me with taking the big picture and resolving it down to the smallest details.

I have long been a proponent of parallel processing and I did take notice of hyper-threading and multi-core processor development. I would like to see massively brachiated parallel processing, but that is because I think that is essential to AI development (among other things). Of course, that doesn't begin to answer the question.

Knowing and understanding the strengths and weaknesses of a particular technology is important for many reasons, beginning with understanding the true costs and benefits of utilizing it. It helps in assessing and planning its implementation, and helps to identify opportunities and potential threats the technology may open doors to.

Because I feel that the proper execution of the process is critical, I do not think it is possible to give one phase importance over any other. A critical failure could result from the failure to properly implement each phase. When implementing a new technology, it is important to ensure that it can integrate well with an existing infrastructure and to know how it will impact other business systems. There will be issues of compatibility, security, efficiency, and applicability to consider, all of which require a strong understanding of what the technology is and is not capable of.

For example, a business might be able to meet all of its IT needs with a network of standardized workstations running Windows, the usual suite of office applications and a common enterprise database software, but the marketing department is requesting Mac workstations because they are better suited for creative work that department needs to do. In spite of improved interfacing between Mac and PC platforms, there remain some issues with compatibility and cost that would not exist if the marketing department compromised and used PC workstations and Windows compatible versions of the desired creative software. Understanding the relative strengths and weaknesses of the two platforms and weighing them against the scope of support each implementation would require from the IT department versus the needs of the marketing department's creative output, would allow a project manager to make a fair ruling about which system was best for the overall needs of the company.

It is absolutely essential to have an established process when implementing a new technology into an organization. Without some kind of organized approach, throwing technology at a problem to fix it is no more effective than simply throwing money at it. In order for new technology to be effective, it has to be implemented based on a clear and precise understanding of the demands it must be able to meet. To do that, it is necessary to begin by assessing and analyzing the business systems in which the technology will be established, the existing infrastructure it must be compatible with, the opportunities it can create, the threats it must be secured against and the costs of implementation. The assessment helps to create a plan that will outline the scope and schedule of implementation, containing the proposed information systems, hardware, software and peripherals and detailing how each meets an established need. If the implementation requires customized hardware or software, the plan will outline the development and testing requirements and put them on the project schedule. Even off the shelf technology needs to be tested for effectiveness and compatibility before installation on a full scale. The best processes employ a waterfall model, to ensure that each stage is completed correctly and to prevent costly errors from occurring. When the time comes to fully implement the new technology, the plan will detail what goes in, where and when, in order to ensure the least disruption of normal business as possible. Sometimes, that will require training for the employees that will be using the new technology prior to the implementation, but it always requires training on the new systems once they are in place and after. The training program is complemented by a maintenance program which includes a vigilant security and disaster recovery program. Implementing technology is a complex dance that requires a lot of choreographing. Without it, chaos is certain to emerge.

I have found myself on both sides of the legacy software issue. Because I've mostly done contract work, there have been a number of dry spells where I could not find an IT job, and while I usually take a part time or temp job to pay the bills the biggest problem I have is that IT skills become outdated very fast. You have to be working in the industry to be able to keep up with it. I am bright enough to pick up on new technology pretty quick on the job, but the longer I go between contracts the more likely it is that I will only be able to win a contract with a client that is using two- or three-year-old technology that does not need technicians with expertise with something too cutting edge for me to have experience with. Because most of my training and experience has been on the job, I chose to return to school to fill in any gaps in my knowledge and even with that it's difficult to qualify for most of the job postings I see. Finally, newer is not always better, as anyone who has struggled with Vista can tell you! I still use Office 2000 because newer versions have removed or distorted features I am comfortable with and would rather have than some of the new features that have been added. Now that I am looking for work again, I am noticing once again how particular job openings are. Most require experience or expertise in areas you can only gain on the workplace. Who has enterprise database software at home to practice on? Who can afford Adobe Create Suite when they are unemployed? Granted, the last I could probably get at a reasonable price through the university, but I would have to buy a new, and much more powerful laptop to use it! The world of computers passes swiftly by anyone caught standing still.

As a contractor I have worked on many IT projects in various different stages and seen the results of good project management and bad, and as you pointed out there is a long list of things that can go wrong. I can remember one instance where I was helping out with an office move for a client I and my project manager had both worked for in the past. My project manager had gotten the contract for the move based on her past record with the client, but on the day of the move she was forced to call in sick because of a severe migraine. She spent the morning hours resting so she would be ready for the move that afternoon, but when she arrived to do the job, her manager pulled her off the job and sent the crew with someone else to lead it. The client was furious when he found out that she had been pulled off the contract, because he had already planned everything out with her and had hired us explicitly because she was the project manager. The substitute was not prepared or qualified for the job and the client called the agency and demanded they send her out and honor their contract, threatening legal action if they did not comply. They were forced to send her out or end up in breach of contract. The move started several hours later than planned because of this, but she was still able to have it done on time because she had everything organized and was a good manager. There was poor communication and office politics involved in this snafu, and after this event all of her subordinates, including me, were laid off on thin excuses in retaliation. The agency was forced to close that office a few months later because of too many similarly bad management decisions. All of this, because the manager of that office wanted to punish an employee for calling in sick for part of her regular work day. As far as I was concerned, she was being a responsible project manager doing what was necessary to fulfill her contract.

The closest I can think of for an IT project I know of "failing" is the one I shared in the discussion question about the project manager who was pulled off a contract for calling sick to get over a migraine. That project did not actually fail, but the consequences of that event could count as a failure. The cause of the problem was personal politics, or rather a retaliation for what was perceived as personal politics; the former working relationship that existed between the project manager and the client prompted the office management to accuse her of instigating the client's threat to sue for breach of contract rather than acknowledge that they made a bad business decision that really would have resulted in a breach of contract. What this event taught me was that, as a project manager, success rides not only on being competent at your job and effective at planning and executing an integration, it is also necessary to be aware of things that affect the circumstances of the implementation and could cause potential problems. One of those is the fact that there are clients and managers and suppliers -- human elements -- who can exert both positive and negative influence on the project. A manager who withholds support, or is dominating or manipulative, or simply incompetent, can become an obstacle to performance and in order to prevent that a project manager has to be proactive, find suitable, legitimate ways to address and resolve personal conflicts that could arise. It is not hard to see that any person can become a weak link, and as is often said, a chain is only as strong as its weakest link. The weak link can be poor planning and execution of a project, inexperienced or incompetent personnel, or haste. The problem underlying the failure rate and wasted effort cited in the study,  is a failure to live up to a simple and honest work ethic, "A  job worth doing is worth doing right." 

I have bounced back and forth between Mac and PC based platforms. In junior high and high school, the systems we had were mostly TRS 80's running DOS, I did most of my early writing on PC clones (8088's and 286's) but my first stint in college I fell in love with the Mac lab. I could not afford to buy one of my own, but I would rent time on Macs or borrow a friends' or roommates' for writing or graphics work, and worked on Macs doing desktop publishing or ad composition. I eventually bought a used 8088 so I could work on a manuscript, then picked up a Toshiba 386 laptop with Windows 3.1 so I could keep working while on a road trip. When I decided to get into IT, most of the time I was working on PC's and the latest version of Windows. I now have far more experience with Windows desktops and servers than I ever had with Macs, but Macs are still so much easier to pick up that I've been able to support them with a fraction of the time I've spent on PCs. I have tried a couple of times to get more experience, but I think I'd have to buy one and use it for a while to get a job as a Mac expert. Having played with Leopard for a few hours, it's really tempting. As you said, there are enough similarities for a good troubleshooter to jump from a PC environment to a Mac, and the suggestions you mentioned are all things I remember having to do to solve the occasional Mac user's problems when they would call in for technical support at my last job. I appreciate you sharing the link for Mac optimization with us. I am going to save that and review it, maybe check out the rest of the site, to add to my Mac troubleshooting skill set.

I remember starting out on BBS's using a slow dial-up modem a couple of years before so the first thing I got involved with on the Internet was IRC. A roommate of mine made friends with a guy on IRC who founded an ISP and introduced us to the world of the Internet. I got interested in web design and gradually worked my way into an internship at that ISP, which was like diving into the deep end. That was where I first used email, and so I started out by creating and administering accounts. We also hosted our customer's web sites and I was the artist in the bunch so I learned the basics of web design creating and maintaining our client and corporate web sites. Before that I had done some desktop publishing and ad composition using Aldus PageMaker and various graphics programs. It did not take long before I was neck deep in everything and loving it, but somehow, in spite of having gotten involved right at the start I got pushed out of the IT industry by the dot com collapse competing against people with masters degrees for tech support jobs. By the time things balanced out, most of the skills I had acquired were out of date. Things quickly shifted from an industry that favored people who adapted to new technology quickly and learned by doing to an industry that required expensive degrees and certificates and used people like toilet paper. The one thing I really like about the IT industry is that it always presents new challenges; there is always something new to learn. The one thing I hate about the IT industry is that it takes very little time for skills and knowledge to become obsolete. You have to have access to the technology to have relevant technology skills, and so you can only ever be as good as the technology you work with.

Before I was given the opportunity to begin working in IT as an intern, I was devoted to creative arts and writing, philosophy and the study of science and metaphysics. I had a lot of creativity, intuition, perception and analytical ability based in critical thinking, and I trained myself to create and develop processes that involved seeing ideas through to completion and approaching them from every angle. My natural ability allows me to think faster, further and deeper than most people around me and it allowed me to jump into the IT world with no more preparation than previous experience using computers and programs available in the late 80's and early 90's. My first day on the job my boss walked me into the workshop where we built, tested and repaired computers and told me to build myself a computer for my desk. Normally, that sort of thing would seem too intimidating to jump into, but it was my first task and the test to see if I was worthy of the position I was being given. To complete that task, I did exactly as you described: I took stock of the materials at hand, I got on the internet and did some research, I talked with the staff and asked questions and listened to their recommendations, I planned out what I needed in terms of parts and software based on what was available off the shelf, assembled and tested my machine, then installed and tested the software, and when it was set up at my desk, I got back online and gathered information to help me train on the software I was unfamiliar with. If I had not considered the relative strengths and weaknesses, the limits and capabilities of each component of a working PC, the end result would have been nothing more than a door-stop.

In my DQ response, I listed the invention of the mouse and the graphic user interface as examples of disruptive technology, so it was a treat to be able to look at the article and get a different perspective on the history of the GUI. I've mentioned before that I have a friend and mentor in IT and I first heard the stories about many of the events in the article from him. I have also come across the occasional article or book and watched documentaries that made mention of things I encountered again in this article by Mark Tuck. It was only as I was reading through the article that I realized that the father of a girlfriend I had in high school had worked at PARC! I remember being at her house and spending a few hours every now and then at her father's workbench playing with his custom built computer getting a sneak peek at programs like Superpaint and Smalltalk. Perhaps if things had gone better with that girlfriend, I might have gotten seriously interested in computers a decade early. I certainly understand what Tuck meant about people not knowing what to do with all of the innovations that had been developed at the Xerox labs. At that time, the technology was fascinating, but it was still rather primitive. Just playing with it gave a person ideas about what could be done, but the big question that followed was always "how?"

I have to appreciate people like Jobs and Gates for taking ideas off the shelf and doing something with them, but then both failed to achieve true greatness because they only moved those ideas to the sales counter. When profit motives take over, the innovation dies, waiting for some other person to see an opportunity laying on the back shelf collecting dust and snags it.

There are many different sides to the downloading controversy, and while I am inclined to agree that the artists deserve to get paid for their work I do not believe that the record companies have the right to exclusive control and profit from distribution. Recording artists have never received their due from the recording labels, most of the money from record sales went to the labels. If the artists were pushing for it, I would support their effort to limit free distribution of music, but with the recording labels making the push I blame them for showing a despicable lack of initiative and consideration for both the artists and their fans. The record companies provide a minimal service, distributing stable, decent quality recordings of live or studio performances. For that reason it is in their interest to promote the artists to the best of their ability, and allowing the free distribution of low quality recordings is an excellent way to do that. People are more likely to purchase a product they have tried out and liked and downloading and sharing songs brings in the most valuable form of advertisement, personal endorsement. The day has come when the distribution of intellectual property is unbounded, so what is being debated now is the question of people's rights to intellectual property and finding a way to acknowledge an individual's share in that property. We are at the point where art and music and literature should have an investment market rather than a distribution market. A song comes out and people go out to buy shares that entitle them to own the rights to a high quality copy and to distribute low quality copies freely to their friends, or shares that entitle a professional entity to broadcast music publicly. Record labels would be like partner companies, still recruiting and promoting artists, helping to produce new content and selling share rights. In an age when people can get access to any information available in an electronic format, the game will go to the entrepreneur who is able to provide the best content on demand with the least amount of complication at the most reasonable cost. Imagine if you could buy the right to download and listen to your favorite song, anywhere and at anytime by purchasing it one time as a stock, at the same time making an investment in the artist and providing him or her with the support to produce the next song. Imagine the same thing of a book or textbook, or a piece of art. This is the age of reproductions. There often are no originals, but there is always an artist. A talent who can be recognized in his or her own time.

The term “Metadata" is usually defined as data about data. Another way to think of it, metadata gives data meaningful parameters, a way of identifying the attributes of a piece of information and tagging it to give the computer pointers about how to associate it. It seems strange to us because we always cross reference information automatically, assigning meaning to words and numbers through context and association. A computer can not know that a piece of information has meaning, but we can tell it that there is an association or link between data and thus a computer can use that tag as a factor to determine how to manipulate the associated data when instructed to. In spite of containing all the information we can cram into them, computers know nothing; they are not capable of deriving meaning from information. Actually, to be more precise, we have not figured out how to design that capacity into them. Processing power alone is not enough. It will take a lot of effort before a computer will be able to free associate enough to attribute meaning to data on its own. Fortunately, we can do that really well!

I still find it astonishing that an effective "practical language" interpreter has not been developed to allow conventional linguistic interface to be used with computers. I think that was one of the first things I tried to develop way back in junior high when my industrial and commercial arts class and strategic and logical thinking class were both focusing on computers and programming. It is very difficult for a person to learn to think like a computer, but if an effective way is developed for a computer to interpret a human language it can be implemented almost instantly on any computer, so even if it seems more difficult it is still the best objective to pursue. The theoretical approach did not take that long to work out, but back when the idea first occurred to me, the technology to do it was not available--but it is now.

Ah, the infamous End User License Agreement! The clear proof that people do not develop software for the good and betterment of humanity. This is just another way that corporations have found to command and control an essential resource that ought to be freely available; information. It is that one step over the line that marks where common sense ends and fascism begins. It is only common sense that people who work very hard to develop software deserve to be paid but it goes beyond reason to sell only the right to use that software under a contractual agreement that absolves the developer of any fault in relation to damage and harm caused by the software and reduces the consumer to the status of tenant on his or her own computer. When there is no alternative but to accept the contract to be able to use the software, it is not surprising that most people do not even bother to read the agreement. There is no room for negotiation, no option for dispute. Not using the software is not really an alternative either. Once software became part of business it ceased to have be an elective decision to use it. The worst thing is, not only are we not gaining ownership of the software we "purchase" are surrendering the property rights of our own personal computers by agreeing to the EULA. At best, the EULA is a necessary evil tolerated to gain access to the tools we now depend on for our livelihood. At worse, the EULA creates a precedent the corporation can use to destroy someone's livelihood with a punitive lawsuit. Would civilization have amounted to anything if hammers and nails remained the property of their designers and manufacturers?

Now, I know I am being a bit extreme here. I am not against licensing in principle, I just think that the industry has gone about it the wrong way here. This approach is one that can not help but discriminate against the poor and downtrodden. It is simply not possible to get blood from a stone and it is to no one's advantage to restrict a person's access to essential infrastructure. I believe that people should rent or own their own hardware, or use publicly available hardware under the library access model. I believe that all software should be free to use with the provision that a rational portion of any profits derived from the use be surrendered to the developers of said software under a practical use license. I believe that software should be designed for safe and responsible use, meaning that IT resources should be secured and restricted according to their potential to do harm in the wrong hands. I believe that secure application protocols should be built in from the ground up, so that every program is authenticated before it is run to ensure that the program has a legitimate source and purpose. I can think of many more ways to ensure that IT is used for the good and betterment of humanity, and not simply as a way to limit and divert wealth and resources to a self-entitled minority.

I feel that freedom of information is an important thing, and having access to information is essential for all of us. For that reason, I am concerned by information commerce. The idea that money is what determines who has access to information is not one bit more reassuring than the idea that people with ill intent can get access to anyone's private data or use that information to help them do harm. The only way I can think of to make things a little less frightening is if a system were in place that required anyone seeking information to register the request, so they are forced to identify themselves, so that the person being investigated can be notified. It would also be prudent to have something in place that would cross reference the user's identity with a database or registry of criminal offenders and alert the authorities of their attempt to gather personal information on a potential victim of a crime. Of course, that would be a challenge to set up and no doubt would be expensive to implement, and there would be difficulty preventing circumvention. When you stop and think about it, computer security has such a long way to go before a truly secure system can be put into place for something as vast and intricate as the Internet. Still, it is possible to limit the amount of exploitation, it's simply a matter of controlling access to information and exercising sound judgment when determining the criteria for giving access to secure information. Money alone should not be the sole determinant of who can get access to vital information.

Even though I am aware of the risks and limitations of electronic storage -- anyone who saw the movie The Net knows that a system that can be tampered with is something to be wary of -- it doesn't change the fact that swift, secure access to medical records can mean the difference between life and death in an emergency. One would hope that each state would implement such a system, or even that a national system could be put in place to ensure that a record is kept of all medical information. As someone who has moved around a great deal, my medical records are scattered over five or six different states and dozens of doctor's offices or hospitals--in two cases there records kept at rehabilitation clinics I visited for minor work related injuries. I feel sorry for any medical professional trying to track that information down and expect that much of it has been lost or discarded because there was nowhere to send it for long term storage and retrieval. I have experienced enough frustration trying to find useful and reliable information on the Internet to wish that there was a career track devoted to creating and maintaining universal information resources capable of providing expert knowledge to anyone. That's an idea I've had in my head since my first flirtation with AI systems in junior high. The technology that did not exist then is starting to appear now. Sadly, while I have a genius for conceptual systems design I utterly lack the technical skills needed to turn my ideas into reality. Never enough time and resources to develop that talent. Still, I could probably do great things working in a think tank supported by real electronic and software engineers! ... Okay, that sounded a little too much like stroking my own feathers, but it is true that the jobs I'm technically qualified for are so far below my level of interest and creativity it's a wonder I still bother to work in the IT field.







